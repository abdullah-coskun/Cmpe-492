{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1pBEJLFXgGd"
   },
   "source": [
    "# RE19-classification: reconstruction of Kurtanovic-Maalej\n",
    "\n",
    "This notebook takes as input the technique presented by Kurtanovic and Maalej at RE'17 (data track), and reconstructs it on the Promise dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4UYLLiWXgGp"
   },
   "source": [
    "## 0. Set up (optional)\n",
    "\n",
    "Run the following  install functions if running Jupyter on a cloud environment like Colaboratory, which does not allow you to install the libraries permanently on your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TMoT1F60XgGy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cython in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.29.13)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.17.2)\n",
      "Requirement already satisfied: benepar[cpu] in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.1.2)\n",
      "Requirement already satisfied: cython in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from benepar[cpu]) (0.29.13)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from benepar[cpu]) (1.17.2)\n",
      "Requirement already satisfied: nltk>=3.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from benepar[cpu]) (3.4.5)\n",
      "Requirement already satisfied: tensorflow>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from benepar[cpu]) (2.0.0a0)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nltk>=3.2->benepar[cpu]) (1.11.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (3.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (0.33.6)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (1.0.8)\n",
      "Requirement already satisfied: google-pasta>=0.1.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (0.1.7)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (0.8.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (0.8.0)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (1.14.0a20190301)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (1.24.1)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (1.14.0.dev2019030115)\n",
      "Requirement already satisfied: gast>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow>=1.11.0->benepar[cpu]) (41.2.0)\n",
      "Requirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow>=1.11.0->benepar[cpu]) (2.10.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow>=1.11.0->benepar[cpu]) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow>=1.11.0->benepar[cpu]) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install cython numpy\n",
    "!pip install benepar[cpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PPplS2cpXgHN"
   },
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4nPAVAEXgHY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/westerops/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/westerops/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/westerops/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package benepar_en2 to\n",
      "[nltk_data]     /Users/westerops/nltk_data...\n",
      "[nltk_data]   Package benepar_en2 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Basic numpy, sklearn, pandas libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "# Basic NLTK tooling\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#benepar.download('benepar_en2')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# The benepar parser -- this is supposed to be a better parser than Stanford's parser used in the RE'17 paper\n",
    "import benepar\n",
    "benepar.download('benepar_en2')\n",
    "\n",
    "# Tqdm, for progress bars -- useful to show that the parsing is working\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5O8ArruqXgHv"
   },
   "source": [
    "## 2. Load data\n",
    "\n",
    "Imports the classified data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J7jMWl49XgHy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ProjectID                                    RequirementText Class  \\\n",
      "0          1  'The system shall refresh the display every 60...    PE   \n",
      "1          1  'The application shall match the color of the ...    LF   \n",
      "2          1  'If projected the data must be readable. On a ...    US   \n",
      "3          1  'The product shall be available during normal ...     A   \n",
      "4          1  'If projected the data must be understandable....    US   \n",
      "\n",
      "   IsFunctional  IsQuality  \n",
      "0             1          1  \n",
      "1             0          1  \n",
      "2             0          1  \n",
      "3             0          1  \n",
      "4             0          1  \n"
     ]
    }
   ],
   "source": [
    "# Loading the re-classified data set PROMISE\n",
    "DATA_FOLDER =  './'\n",
    "#data = pd.read_csv(DATA_FOLDER+'promise-reclass.csv', engine='python')\n",
    "data = pd.read_csv('promise-reclass.csv', engine='python')\n",
    "print (data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MBTYma3AXgIE"
   },
   "source": [
    "## 3. Dataset enrichment\n",
    "\n",
    "Additional features are added automatically, as per the RE'17 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLmhUSlmXgIJ"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'GraphDef'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-881da669527d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# POS tags and tree information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenepar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'benepar_en2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Modal'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Adjective'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/benepar/nltk_plugin.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, batch_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMaximum\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0msentences\u001b[0m \u001b[0mto\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0mper\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \"\"\"\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_LOOKUP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_language_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/benepar/base_parser.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, batch_size)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'GraphDef'"
     ]
    }
   ],
   "source": [
    "# Text length\n",
    "data['Length'] = 0\n",
    "idx = 0\n",
    "for x in data['RequirementText']:\n",
    "    data.at[idx, 'Length'] = len(x)\n",
    "    idx = idx + 1  \n",
    "\n",
    "# POS tags and tree information\n",
    "parser = benepar.Parser('benepar_en2')\n",
    "data['Modal'] = 0.0\n",
    "data['Adjective'] = 0.0\n",
    "data['Noun'] = 0.0\n",
    "data['Adverb'] = 0.0\n",
    "data['Verb'] = 0.0\n",
    "data['TreeHeight'] = 0\n",
    "data['SubTrees'] = 0\n",
    "idx = 0\n",
    "for req in tqdm(data['RequirementText'], desc='Parse trees', position=0):\n",
    "    tokens = tokenizer.tokenize(req)\n",
    "    data.at[idx, 'Words'] = len(tokens)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    fd = nltk.FreqDist(tag for (word, tag) in tags)\n",
    "    for key, value in fd.items():\n",
    "        if key==\"MD\":\n",
    "            data.at[idx, 'Modal'] = value\n",
    "        if key.startswith(\"JJ\"):\n",
    "            data.at[idx, 'Adjective'] = value\n",
    "        if key.startswith(\"VB\"):\n",
    "            data.at[idx, 'Verb'] = value\n",
    "        if key.startswith(\"NN\"):\n",
    "            data.at[idx, 'Noun'] = value\n",
    "        if key==\"RB\":\n",
    "            data.at[idx, 'Adverb'] = value\n",
    "    data.at[idx, 'Modal'] = data.at[idx, 'Modal'] / len(tokens)\n",
    "    data.at[idx, 'Adjective'] = data.at[idx, 'Adjective'] / len(tokens)\n",
    "    data.at[idx, 'Noun'] = data.at[idx, 'Noun'] / len(tokens)\n",
    "    data.at[idx, 'Adverb'] = data.at[idx, 'Adverb'] / len(tokens)\n",
    "    data.at[idx, 'Verb'] = data.at[idx, 'Verb'] / len(tokens)       \n",
    "    tree = parser.parse(req)\n",
    "    data.at[idx, 'TreeHeight'] = tree.height()\n",
    "    data.at[idx, 'SubTrees'] = len(tree)\n",
    "    idx = idx + 1    \n",
    "    \n",
    "print(data[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9cYs59nXgIe"
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "bigrams = []\n",
    "trigrams = []\n",
    "frequencies = Counter([])\n",
    "frequencies2 = Counter([])\n",
    "frequencies3 = Counter([])\n",
    "pfrequencies = Counter([])\n",
    "pfrequencies2 = Counter([])\n",
    "pfrequencies3 = Counter([])\n",
    "\n",
    "wn_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Generation of [1, 2, 3] textgrams, [1, 2, 3] POSgrams\n",
    "# Fix with: tokenize, remove stopwords, lemmatize, then \n",
    "for req in tqdm(data['RequirementText'], desc='n-grams generation', position=0):\n",
    "    token = tokenizer.tokenize(req)\n",
    "    token = [word.lower() for word in token]\n",
    "    tags = nltk.pos_tag(token)\n",
    "    token = [w for w in token if not w in stop_words.ENGLISH_STOP_WORDS]\n",
    "    token = [wn_lemmatizer.lemmatize(w) for w in token]\n",
    "    frequencies += Counter(token)\n",
    "    bigrams = ngrams(token,2)\n",
    "    trigrams = ngrams(token,3)\n",
    "    frequencies2 += Counter(bigrams)\n",
    "    frequencies3 += Counter(trigrams)\n",
    "    punigrams = [tag for (word, tag) in tags]\n",
    "    pfrequencies += Counter(punigrams)\n",
    "    pbigrams = ngrams([tag for (word, tag) in tags], 2)\n",
    "    pfrequencies2 += Counter(pbigrams)\n",
    "    ptrigrams = ngrams([tag for (word, tag) in tags], 3)\n",
    "    pfrequencies3 += Counter(ptrigrams)\n",
    "\n",
    "# Labeling of the features\n",
    "for f in list(frequencies):\n",
    "  label = '_' + f + '_'\n",
    "  data[label] = 0\n",
    "\n",
    "for f in list(frequencies2):\n",
    "  label = '_' + f[0] + '_' + f[1] + '_'\n",
    "  data[label] = 0\n",
    "\n",
    "for f in list(frequencies3):\n",
    "  label = '_' + f[0] + '_' + f[1] + '_' + f[2] + '_'\n",
    "  data[label] = 0\n",
    "\n",
    "for f in list(pfrequencies):\n",
    "  label = f\n",
    "  data[label] = 0\n",
    "  \n",
    "for f in list(pfrequencies2):\n",
    "  label = f[0] + '_' + f[1]\n",
    "  data[label] = 0\n",
    "\n",
    "for f in list(pfrequencies3):\n",
    "  label = f[0] + '_' + f[1] + '_' + f[2]\n",
    "  data[label] = 0\n",
    "  \n",
    "print (len(frequencies), len(frequencies2), len(frequencies3), len(pfrequencies), len(pfrequencies2), len(pfrequencies3))\n",
    "\n",
    "# Populating the n-grams\n",
    "idx = 0\n",
    "for req in tqdm(data['RequirementText'], desc='n-grams population', position=0):\n",
    "    token = tokenizer.tokenize(req)\n",
    "\n",
    "    for t in token:\n",
    "      exists = [col for col in data.columns if col == str('_' + t + '_')]\n",
    "      if exists != []:\n",
    "        data.at[idx, exists] = 1\n",
    "      \n",
    "    bigrams = ngrams(token,2)\n",
    "    for bg in bigrams:\n",
    "      exists = [col for col in data.columns if col == str('_' + bg[0] + '_' + bg[1] + '_')]\n",
    "      if exists != []:\n",
    "        data.at[idx, exists] = 1\n",
    "    \n",
    "    trigrams = ngrams(token,3)\n",
    "    for tg in trigrams:\n",
    "      exists = [col for col in data.columns if col == str('_' + tg[0] + '_' + tg[1] + '_' + tg[2] + '_')]\n",
    "      if exists != []:\n",
    "        data.at[idx, exists] = 1\n",
    "    \n",
    "    tags = nltk.pos_tag(token)\n",
    "\n",
    "    for t in tags:\n",
    "      exists = [col for col in data.columns if col == str(t)]\n",
    "      if exists != []:\n",
    "        data.at[idx, exists] = 1\n",
    "        \n",
    "    pbigrams = ngrams([tag for (word, tag) in tags], 2)\n",
    "    for bg in pbigrams:\n",
    "      exists = [col for col in data.columns if col == str(bg[0] + '_' + bg[1])]\n",
    "      if exists != []:\n",
    "        data.at[idx, exists] = 1\n",
    "\n",
    "    ptrigrams = ngrams([tag for (word, tag) in tags], 3)\n",
    "    for tg in ptrigrams:\n",
    "      exists = [col for col in data.columns if col == str(tg[0] + '_' + tg[1] + '_' + tg[2])]\n",
    "      if exists != []:\n",
    "        data.at[idx, exists] = 1\n",
    "    \n",
    "    idx = idx + 1\n",
    "\n",
    "data.columns = data.columns.map(str)\n",
    "\n",
    "print (data.head())\n",
    "\n",
    "# The enriched dataset is now saved\n",
    "data.to_csv('dataset-full.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zRTVSSpsXgJA"
   },
   "source": [
    "## 4. Feature reduction\n",
    "\n",
    "We reduce the dimensionality of the data. Change the *target*  parameter in the second cell to determine whether you want to train a classifier for F, Q, only F, or only Q requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f6RqNFqjXgJ1"
   },
   "outputs": [],
   "source": [
    "# Creation of an ensemble that uses adaptive boost, gradient boos, extra trees, and random forest\n",
    "def createTop (nfeatures, data, X_train, y_train, target):\n",
    "  #nfeatures = 100\n",
    "\n",
    "  ada_boost_clf = AdaBoostClassifier(random_state=42, n_estimators=30)\n",
    "  ada_boost_clf.fit(X_train, y_train)\n",
    "\n",
    "  gradient_boost_clf = GradientBoostingClassifier(random_state=42, n_estimators=30, max_depth = 5)\n",
    "  gradient_boost_clf.fit(X_train, y_train)\n",
    "\n",
    "  extra_trees_clf = ExtraTreesClassifier(random_state=42, n_estimators=30, max_depth = 5)\n",
    "  extra_trees_clf.fit(X_train, y_train)\n",
    "\n",
    "  random_forest_clf = RandomForestClassifier(random_state=42, n_estimators=30, max_depth = 5)\n",
    "  random_forest_clf.fit(X_train, y_train)\n",
    "\n",
    "  # Sorting in order of importance: average importance\n",
    "  importances = ada_boost_clf.feature_importances_  + gradient_boost_clf.feature_importances_ + extra_trees_clf.feature_importances_ + random_forest_clf.feature_importances_\n",
    "  indices = np.argsort(importances)[::-1]\n",
    "\n",
    "  # Print the feature ranking\n",
    "  print(\"Feature ranking:\")\n",
    "\n",
    "  tokeep = []\n",
    "  for f in range(0, nfeatures):\n",
    "      print(\"%d. feature %s (%f)\" % (f + 1, X_train.columns[indices[f]], importances[indices[f]]))\n",
    "      tokeep.append(X_train.columns[indices[f]])\n",
    "\n",
    "  tokeep.append('RequirementText')\n",
    "  tokeep.append('ProjectID')\n",
    "  tokeep.append('Class')\n",
    "  if target=='OnlyQuality':\n",
    "    tokeep.append('OnlyQuality')\n",
    "    tokeep.append('IsFunctional')\n",
    "    appendix = 'oq'\n",
    "  elif target=='OnlyFunctional':\n",
    "    tokeep.append('IsQuality')\n",
    "    tokeep.append('OnlyFunctional')\n",
    "    appendix = 'of'\n",
    "  elif target=='IsQuality' or target=='IsFunctional':\n",
    "    tokeep.append('IsQuality')\n",
    "    tokeep.append('IsFunctional')\n",
    "    if target=='IsQuality':\n",
    "      appendix = 'q'\n",
    "    else:\n",
    "      appendix = 'f'\n",
    "\n",
    "  data3 = data[tokeep]\n",
    "\n",
    "  print (data3.head())\n",
    "  data3.to_csv('promise-km-' + str(nfeatures) + '-' + appendix + '.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WRzoz1KjXgJJ"
   },
   "outputs": [],
   "source": [
    "# Set the target: choose between IsFunctional, IsQuality, OnlyFunctional, OnlyQuality\n",
    "target = 'OnlyQuality'\n",
    "\n",
    "data = pd.read_csv('dataset-full.csv', engine='python')\n",
    "datarep = data.drop(data.columns[0], axis=1)\n",
    "\n",
    "if target=='OnlyQuality':\n",
    "  datarep['OnlyQuality'] = ~datarep['IsFunctional'] & datarep['IsQuality']\n",
    "  todrop = ['RequirementText', 'Class', 'ProjectID', 'IsFunctional', 'IsQuality']\n",
    "\n",
    "if target=='OnlyFunctional':\n",
    "  datarep['OnlyFunctional'] = datarep['IsFunctional'] & ~datarep['IsQuality']\n",
    "  todrop = ['RequirementText', 'Class', 'ProjectID', 'IsFunctional', 'IsQuality']\n",
    "\n",
    "if target=='IsQuality':\n",
    "  todrop = ['RequirementText', 'Class', 'ProjectID', 'IsFunctional']\n",
    "\n",
    "if target=='IsFunctional':\n",
    "  todrop = ['RequirementText', 'Class', 'ProjectID', 'IsQuality']\n",
    "\n",
    "\n",
    "# Remove the features that are not used for the classification\n",
    "data2 = datarep.drop(todrop, axis = 1)\n",
    "\n",
    "# Create training and testing set\n",
    "# === BEGIN REMOVED AFTER CONDITIONAL ACCEPT\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#      data2.drop([target], axis=1), data2[target], test_size=0.25, random_state=42)\n",
    "# print (X_train.columns)\n",
    "# === END REMOVED AFTER CONDITIONAL ACCEPT\n",
    "\n",
    "\n",
    "# === BEGIN REMOVED AFTER CONDITIONAL ACCEPT\n",
    "# createTop (500, datarep, X_train, y_train, target)\n",
    "# === END REMOVED AFTER CONDITIONAL ACCEPT\n",
    "\n",
    "# === BEGIN ADDED AFTER CONDITIONAL ACCEPT\n",
    "createTop (100, datarep, data2.drop([target], axis=1), data2[target], target)\n",
    "# === BEGIN REMOVED  AFTER CONDITIONAL ACCEPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rynuLsrWXgKN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_KM_reconstruction.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

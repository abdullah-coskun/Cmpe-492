{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAs4pYb2DbRL"
   },
   "source": [
    "# RE19- Linguistic datasets generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9IfLwN8DbRa"
   },
   "source": [
    "The notebook takes in input a list of datasets and enrich them with linguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 937
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20880,
     "status": "ok",
     "timestamp": 1554724431363,
     "user": {
      "displayName": "Davide Dell'Anna",
      "photoUrl": "https://lh5.googleusercontent.com/-dJU__MJ08Ww/AAAAAAAAAAI/AAAAAAAAbsA/YkubKaeFgQo/s64/photo.jpg",
      "userId": "01259263475584376287"
     },
     "user_tz": -120
    },
    "id": "0CccMh_HDdRO",
    "outputId": "9ac15ec1-2a4e-4293-ba36-61213f05cd20",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cython in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.29.13)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.17.2)\n",
      "Requirement already satisfied: skope-rules in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.0.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from skope-rules) (1.3.1)\n",
      "Requirement already satisfied: pandas>=0.18.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from skope-rules) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from skope-rules) (1.17.2)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from skope-rules) (0.21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas>=0.18.1->skope-rules) (2.7.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas>=0.18.1->skope-rules) (2018.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn>=0.17.1->skope-rules) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.18.1->skope-rules) (1.11.0)\n",
      "Requirement already satisfied: benepar[cpu] in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.1.2)\n",
      "Requirement already satisfied: cython in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from benepar[cpu]) (0.29.13)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from benepar[cpu]) (1.17.2)\n",
      "Requirement already satisfied: nltk>=3.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from benepar[cpu]) (3.4.5)\n",
      "Requirement already satisfied: tensorflow>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from benepar[cpu]) (2.0.0)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nltk>=3.2->benepar[cpu]) (1.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (3.10.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (0.33.6)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (0.8.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (0.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (2.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (1.11.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (1.24.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (3.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (0.1.7)\n",
      "Requirement already satisfied: gast==0.2.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (0.2.2)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow>=1.11.0->benepar[cpu]) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow>=1.11.0->benepar[cpu]) (41.2.0)\n",
      "Requirement already satisfied: h5py in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow>=1.11.0->benepar[cpu]) (2.10.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow>=1.11.0->benepar[cpu]) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow>=1.11.0->benepar[cpu]) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install cython numpy\n",
    "!pip3 install skope-rules\n",
    "!pip3 install benepar[cpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUB0B6y2DbRk"
   },
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8987,
     "status": "ok",
     "timestamp": 1554724443809,
     "user": {
      "displayName": "Davide Dell'Anna",
      "photoUrl": "https://lh5.googleusercontent.com/-dJU__MJ08Ww/AAAAAAAAAAI/AAAAAAAAbsA/YkubKaeFgQo/s64/photo.jpg",
      "userId": "01259263475584376287"
     },
     "user_tz": -120
    },
    "id": "FFq2ysr7DbRw",
    "outputId": "adcbafa2-09d7-464c-c371-b03a73ca263c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1076)>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
      "[nltk_data]     failed: unable to get local issuer certificate\n",
      "[nltk_data]     (_ssl.c:1076)>\n",
      "[nltk_data] Error loading benepar_en2: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1076)>\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LEMMA_INDEX' from 'spacy.lang.en' (/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/spacy/lang/en/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-910ef701c5b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLEMMA_INDEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA_EXC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA_RULES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLEMMA_INDEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA_EXC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA_RULES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LEMMA_INDEX' from 'spacy.lang.en' (/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/spacy/lang/en/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import skope-rules\n",
    "from skrules import SkopeRules\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "\n",
    "#Import basic NLTK tooling\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#Import benepar parser\n",
    "import benepar\n",
    "benepar.download('benepar_en2')\n",
    "\n",
    "#Tqdm, for the progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#from spacy.lemmatizer import Lemmatizer\n",
    "#from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "#lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4uFH02IVDbR9"
   },
   "source": [
    "## 2. Constants and functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wavc8Vu4-pId",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getDependenciesFeaturesSets(type):\n",
    "  \"\"\"\n",
    "  Retrieves the set of linguistic features appropriate, based on the name of the feature set given as input.\n",
    "  Nine groups of features, calculated in the notebook 04_ling_stats_calculator, are considered in this function:\n",
    "  1. single dependencies\n",
    "  2. combinations of 2 dependencies\n",
    "  3. combinations of 3 dependencies\n",
    "  4. single branches\n",
    "  5. combinations of 2 branches\n",
    "  6. combinations of 3 branches\n",
    "  7. sequences of POSdep\n",
    "  8. combinations of 2 sequences of POSdep\n",
    "  9. combinations of 3 sequences of POSdep\n",
    "  The following feature sets are defined:\n",
    "  all: the top 10 features for each of groups 1-6, no features from group 7 (TOGETHER WITH ext IS USED IN THE PAPER AS FS3)\n",
    "  sd: the single dependencies that appeared at least once in the top 10 features of groups 1-3, no feat from group 7 (USED IN THE PAPER AS FS1)\n",
    "  sdsb: same as sd but for groups 1-6\n",
    "  sdsb8sel02: the features with delta>0.2 in sdsb (TOGETHER WITH ext IS USED IN THE PAPER AS FS2)\n",
    "  seq: the features in sdsb8sel02 + the top 10 features of groups 7-9\n",
    "  ev: a merge of all the previous ones\n",
    "  two: only dobj and nummod\n",
    "  FinalSel: a final selection of 11 features (extended with 4 additional ones, outside this function is USED IN THE PAPER AS THE FINAL SET)\n",
    "  N.B. the suffix ext refers to additional features added outside of this function\n",
    "  @param type: one of the names of the feature sets above defined\n",
    "  @return: the appropriate lists of features\n",
    "  \"\"\"\n",
    "  if type=='all' or type=='allext':\n",
    "    significant_dependencies = ['dobj', 'nummod', 'acl', 'amod', 'auxpass',\n",
    "                                'advmod', 'nsubjpass', 'nsubj', 'nmod', 'advcl']\n",
    "    significant_2dependencies = [['ROOT','nummod'], ['aux','nummod'], ['det','nummod'], \n",
    "                                  ['nummod','punct'], ['ROOT','dobj'], ['aux','dobj'], \n",
    "                                  ['nummod','pobj'], ['nsubj','dobj'], ['nsubj','nummod'], ['dobj','pobj']]\n",
    "    significant_3dependencies = [['ROOT','nummod','punct'], ['aux','ROOT','nummod'], ['aux','nummod','punct'],\n",
    "                                ['det','ROOT','nummod'], ['det','nummod','punct'], ['det','aux','nummod'], \n",
    "                                ['ROOT','det','dobj'], ['nsubj','det','dobj'], ['aux','det','dobj'], \n",
    "                                ['nsubj','aux','dobj']]\n",
    "\n",
    "    significant_branches = ['ROOT_dobj_det','ROOT_dobj_acl_aux','ROOT_dobj_acl_dobj_det',\n",
    "                          'ROOT_prep_pobj_det','ROOT_auxpass','ROOT_prep_pobj_compound','ROOT_nsubj',\n",
    "                          'ROOT_ccomp_aux', 'ROOT_nsubj_nummod', 'ROOT_prep_pobj_nummod']\n",
    "\n",
    "    significant_2branches = [['ROOT_dobj_det','ROOT_nsubj_det'],\n",
    "                            ['ROOT_aux','ROOT_dobj_det'],\n",
    "                            ['ROOT_dobj_det','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_aux '],\n",
    "                            ['ROOT_punct','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_dobj_acl_aux'],\n",
    "                            ['ROOT_dobj_acl_aux','ROOT_dobj_det'],\n",
    "                            ['ROOT_dobj_acl_aux','ROOT_punct'],\n",
    "                            ['ROOT_dobj_acl_aux','ROOT_nsubj_det'],\n",
    "                            ['ROOT_prep_pobj_det','ROOT_punct']]\n",
    "\n",
    "    significant_3branches = [['ROOT_aux','ROOT_dobj_det','ROOT_nsubj_det'],\n",
    "                            ['ROOT_dobj_det','ROOT_nsubj_det','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_dobj_det','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_aux','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_punct','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_aux','ROOT_nsubj_det'],\n",
    "                            ['ROOT_nsubj_det','ROOT_punct','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_dobj_acl_aux','ROOT_dobj_det'],\n",
    "                            ['ROOT_aux','ROOT_dobj_acl_aux','ROOT_punct'], \n",
    "                           ['ROOT_dobj_acl_aux','ROOT_dobj_det','ROOT_punct']]\n",
    "    \n",
    "    significant_sequences = []\n",
    "    \n",
    "  elif type=='sd' or type=='sdext':\n",
    "    significant_dependencies = ['dobj', 'nummod', 'acl', 'amod', 'auxpass',\n",
    "                              'advmod', 'nsubjpass', 'nsubj', 'nmod', 'aux', 'pobj', 'prep', 'det', 'punct']\n",
    "    significant_2dependencies = []\n",
    "    significant_3dependencies = []\n",
    "    significant_branches = []\n",
    "    significant_2branches = []\n",
    "    significant_3branches = []\n",
    "    significant_sequences = []\n",
    "  elif type=='sdsb' or type=='sdsbext':\n",
    "    significant_dependencies = ['dobj', 'nummod', 'acl', 'amod', 'auxpass',\n",
    "                                'advmod', 'nsubjpass', 'nsubj', 'nmod', 'aux', 'pobj', 'prep', 'det', 'punct']\n",
    "    significant_2dependencies = []\n",
    "    significant_3dependencies = []\n",
    "    significant_branches = ['ROOT_dobj_det','ROOT_dobj_acl_aux','ROOT_prep_pobj_det','ROOT_acomp_xcomp_aux','ROOT_nsubjpass_det',\n",
    "                            'ROOT_dobj_acl_dobj_det','ROOT_prep_pobj_compound','ROOT_acomp_xcomp_dobj_det','ROOT_nsubj_det','ROOT_dobj_acl_aux']\n",
    "    significant_2branches = []\n",
    "    significant_3branches = []\n",
    "    significant_sequences = []\n",
    "  elif type=='sdsb8sel02' or type=='sdsb8sel02ext':\n",
    "    significant_dependencies = ['dobj', 'acl', 'prep', 'det', 'pobj','aux', 'nsubj', 'punct']\n",
    "    significant_2dependencies = []\n",
    "    significant_3dependencies = []\n",
    "    significant_branches = ['ROOT_aux','ROOT_dobj_det','ROOT_punct','ROOT_nsubj_det']\n",
    "    significant_2branches = []\n",
    "    significant_3branches = []\n",
    "    significant_sequences = []\n",
    "  elif type=='seq' or type=='seqext':\n",
    "    significant_dependencies = ['dobj', 'acl', 'prep', 'det', 'pobj','aux', 'nsubj', 'punct']\n",
    "    significant_2dependencies = []\n",
    "    significant_3dependencies = []\n",
    "    significant_branches = ['ROOT_aux','ROOT_dobj_det','ROOT_punct','ROOT_nsubj_det']\n",
    "    significant_2branches = []\n",
    "    significant_3branches = []\n",
    "    significant_sequences = ['NNdobj', 'TOaux', 'NNPnsubj', 'RBadvmod', 'VBxcomp', 'VBauxpass', 'CDnummod', 'VBROOT', 'NNdobj_INprep', 'VBROOT_DTdet', 'DTdet_NNdobj', 'MDaux_VBROOT', 'JJacomp_TOaux', 'NNPnsubj_MDaux',\n",
    "                            'MDaux_VBROOT_DTdet', 'VBROOT_DTdet_NNdobj', 'JJacomp_TOaux_VBxcomp', 'VBROOT_JJacomp_TOaux', 'MDaux_VBROOT_DTdet_NNdobj', 'MDaux_VBROOT_JJacomp_TOaux', 'VBROOT_JJacomp_TOaux_VBxcomp',\n",
    "                            'MDaux', 'DTdet', 'NNnsubj', 'INprep']\n",
    "  elif type=='ev' or type=='evext':\n",
    "    significant_dependencies = ['dobj', 'nummod', 'acl', 'amod', 'auxpass',\n",
    "                                'advmod', 'nsubjpass', 'nsubj', 'nmod', 'advcl', 'prep', 'det','pobj', 'aux', 'punct']\n",
    "    significant_2dependencies = [['ROOT','nummod'], ['aux','nummod'], ['det','nummod'], \n",
    "                                  ['nummod','punct'], ['ROOT','dobj'], ['aux','dobj'], \n",
    "                                  ['nummod','pobj'], ['nsubj','dobj'], ['nsubj','nummod'], ['dobj','pobj']]\n",
    "    significant_3dependencies = [['ROOT','nummod','punct'], ['aux','ROOT','nummod'], ['aux','nummod','punct'],\n",
    "                                ['det','ROOT','nummod'], ['det','nummod','punct'], ['det','aux','nummod'], \n",
    "                                 ['ROOT','det','dobj'], ['nsubj','det','dobj'], ['aux','det','dobj'], \n",
    "                                 ['nsubj','aux','dobj']]\n",
    "\n",
    "    significant_branches = ['ROOT_dobj_det', 'ROOT_acomp_xcomp_aux','ROOT_nsubjpass_det',\n",
    "                            'ROOT_acomp_xcomp_dobj_det','ROOT_nsubj_det',\n",
    "                            'ROOT_dobj_acl_aux',\n",
    "                            'ROOT_dobj_acl_dobj_det',\n",
    "                            'ROOT_prep_pobj_det',\n",
    "                            'ROOT_auxpass',\n",
    "                            'ROOT_prep_pobj_compound',\n",
    "                            'ROOT_nsubj',\n",
    "                            'ROOT_ccomp_aux',\n",
    "                            'ROOT_nsubj_nummod',\n",
    "                            'ROOT_prep_pobj_nummod', 'ROOT_aux', 'ROOT_punct']\n",
    "\n",
    "    significant_2branches = [['ROOT_dobj_det','ROOT_nsubj_det'],\n",
    "                            ['ROOT_aux','ROOT_dobj_det'],\n",
    "                            ['ROOT_dobj_det','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_aux '],\n",
    "                            ['ROOT_punct','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_dobj_acl_aux'],\n",
    "                            ['ROOT_dobj_acl_aux','ROOT_dobj_det'],\n",
    "                            ['ROOT_dobj_acl_aux','ROOT_punct'],\n",
    "                            ['ROOT_dobj_acl_aux','ROOT_nsubj_det'],\n",
    "                            ['ROOT_prep_pobj_det','ROOT_punct']]\n",
    "\n",
    "    significant_3branches = [['ROOT_aux','ROOT_dobj_det','ROOT_nsubj_det'],\n",
    "                            ['ROOT_dobj_det','ROOT_nsubj_det','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_dobj_det','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_aux','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_punct','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_aux','ROOT_nsubj_det'],\n",
    "                            ['ROOT_nsubj_det','ROOT_punct','ROOT_punct'],\n",
    "                            ['ROOT_aux','ROOT_dobj_acl_aux','ROOT_dobj_det'],\n",
    "                            ['ROOT_aux','ROOT_dobj_acl_aux','ROOT_punct'], ['ROOT_dobj_acl_aux','ROOT_dobj_det','ROOT_punct']]\n",
    "    \n",
    "    significant_sequences = ['NNdobj', 'TOaux', 'NNPnsubj', 'RBadvmod', 'VBxcomp', 'VBauxpass', 'CDnummod', 'VBROOT', 'NNdobj_INprep', 'VBROOT_DTdet', 'DTdet_NNdobj', 'MDaux_VBROOT', 'JJacomp_TOaux', 'NNPnsubj_MDaux',\n",
    "                            'MDaux_VBROOT_DTdet', 'VBROOT_DTdet_NNdobj', 'JJacomp_TOaux_VBxcomp', 'VBROOT_JJacomp_TOaux', 'MDaux_VBROOT_DTdet_NNdobj', 'MDaux_VBROOT_JJacomp_TOaux', 'VBROOT_JJacomp_TOaux_VBxcomp',\n",
    "                            'MDaux', 'DTdet', 'NNnsubj', 'INprep']\n",
    "  elif type=='two':\n",
    "    significant_dependencies = ['dobj', 'nummod']\n",
    "    significant_2dependencies = []\n",
    "    significant_3dependencies = []\n",
    "    significant_branches = []\n",
    "    significant_2branches = []\n",
    "    significant_3branches = []\n",
    "    significant_sequences = []\n",
    "    \n",
    "  elif 'FinalSel' in type:\n",
    "    significant_dependencies = ['nsubj', 'dobj', 'nummod', 'amod', 'acl', 'nmod', 'auxpass', 'nsubjpass', 'prep', 'pobj', 'advmod']\n",
    "    significant_2dependencies = []\n",
    "    significant_3dependencies = []\n",
    "    significant_branches = []\n",
    "    significant_2branches = []\n",
    "    significant_3branches = []\n",
    "    significant_sequences = []\n",
    "    \n",
    "  else:\n",
    "    significant_dependencies = []\n",
    "    significant_2dependencies = []\n",
    "    significant_3dependencies = []\n",
    "    significant_branches = []\n",
    "    significant_2branches = []\n",
    "    significant_3branches = []\n",
    "    significant_sequences = []\n",
    "    \n",
    "    \n",
    "  return significant_dependencies, significant_2dependencies, significant_3dependencies, significant_branches, significant_2branches, significant_3branches, significant_sequences\n",
    "\n",
    "\n",
    "\n",
    "def get_all_paths(node, h, max_h):\n",
    "    \"\"\"\n",
    "    Calculates all the dependencies paths (branches) in a requirement dependency tree up to an height of max_h\n",
    "    @param node: the root of the tree\n",
    "    @param h: the initial height (typically 0)\n",
    "    @return: a list of strings representing paths\n",
    "    \"\"\"\n",
    "    if node.n_lefts + node.n_rights == 0 or h==max_h:\n",
    "        return [node.dep_]\n",
    "    return [\n",
    "        node.dep_ + '_' + str(path) for child in node.children for path in get_all_paths(child, h+1, max_h)\n",
    "    ]\n",
    "\n",
    "\n",
    "def createEnrichedDataset(data, new_file_name, dep_feat_type):\n",
    "    \"\"\"\n",
    "    Creates a <new_file_name>.csv file with dataset data enriched with the features in dep_feat_type\n",
    "    @param data: the original dataset\n",
    "    @param new_file_name: the name of the new dataset\n",
    "    @param dep_feat_type: the type of feature sets, see function getDependenciesFeaturesSets for a description\n",
    "    \"\"\"\n",
    "    \n",
    "    columns_to_keep = ['ProjectID','RequirementText','Class','IsFunctional','IsQuality']\n",
    "    for c in data.columns:\n",
    "        if not c in columns_to_keep:\n",
    "            data = data.drop(c, axis = 1)\n",
    "   \n",
    "    # the presence of ext in dep_feat_type indicates that we want to extend the features obtained from function getDependenciesFeaturesSets \n",
    "    # with additional features from literature\n",
    "    if \"ext\" in dep_feat_type:\n",
    "      data['Length'] = 0\n",
    "      idx = 0\n",
    "      for x in data['RequirementText']:\n",
    "          data.at[idx, 'Length'] = len(x)\n",
    "          idx = idx + 1\n",
    "      data['AdvMod'] = 0\n",
    "      data['AMod'] = 0\n",
    "      data['AComp'] = 0\n",
    "      data['DTreeHeight'] = 0\n",
    "      \n",
    "    if \"FinalSel\" in dep_feat_type:\n",
    "      data['AComp'] = 0\n",
    "      \n",
    "    # get the features to use \n",
    "    significant_dependencies, significant_2dependencies, significant_3dependencies, significant_branches, significant_2branches, significant_3branches, significant_sequences = getDependenciesFeaturesSets(dep_feat_type)\n",
    "    \n",
    "    # init columns of the dataframe for the appropriate features\n",
    "    for d in significant_dependencies:\n",
    "        data[d] = 0\n",
    "    for c in significant_2dependencies:\n",
    "        data[c[0]+'+'+c[1]] = 0\n",
    "    for t in significant_3dependencies:\n",
    "        data[t[0]+'+'+t[1]+'+'+t[2]] = 0\n",
    "    for d in significant_branches:\n",
    "        data[d] = 0\n",
    "    for c in significant_2branches:\n",
    "        data[c[0]+'+'+c[1]] = 0\n",
    "    for t in significant_3branches:\n",
    "        data[t[0]+'+'+t[1]+'+'+t[2]] = 0\n",
    "    for s in significant_sequences:\n",
    "      data[s] = 0\n",
    "\n",
    "    # loop for all rows in the original dataset\n",
    "    idx = 0\n",
    "    for req in tqdm(data['RequirementText'], desc='spaCy analysis', position=0):\n",
    "        token = tokenizer.tokenize(req)\n",
    "        doc = nlp(req)\n",
    "        printed = False\n",
    "        maxHeight = 1\n",
    "        req_dep = []\n",
    "        req_tagged_seq = ''\n",
    "        for t in doc:\n",
    "            req_dep.append(t.dep_)\n",
    "            req_tagged_seq = req_tagged_seq+t.tag_+t.dep_+\"_\"\n",
    "\n",
    "        dep_br_lists = [get_all_paths(sent.root, 0, 15) for sent in doc.sents]\n",
    "        dep_br = []\n",
    "        for l in dep_br_lists:\n",
    "            if l!=['ROOT']:\n",
    "                dep_br = dep_br + l\n",
    "        dep_br.sort()\n",
    "\n",
    "        if \"ext\" in dep_feat_type:\n",
    "          for sent in doc.sents:\n",
    "              for token in sent:\n",
    "                  height = 1\n",
    "                  for t in token.ancestors:\n",
    "                      height = height + 1\n",
    "                  if height > maxHeight:\n",
    "                      maxHeight = height\n",
    "\n",
    "                  # TODO: Limit to Root verb?\n",
    "                  if token.dep_ == 'advmod' and token.head.pos_ == 'VERB' and token.pos_ == 'ADV':\n",
    "                      #print('Pattern 1: VB', token.head, '->', token.dep_, '-> RB', token.text)\n",
    "                      data.at[idx, 'AdvMod'] = data.at[idx, 'AdvMod'] + 1\n",
    "\n",
    "                  if token.dep_ == 'amod' and token.head.pos_ == 'NOUN' and token.pos_ == 'ADJ':\n",
    "                      # Could be made stronger by making the head traversal recursive \n",
    "                      if token.head.dep_ == 'nsubj':\n",
    "                          continue\n",
    "                      #print('Pattern 2: NN', token.head, '->', token.dep_, '-> ADJ', token.text)  \n",
    "                      data.at[idx, 'AMod'] = data.at[idx, 'AMod'] + 1\n",
    "\n",
    "                  if token.dep_ == 'acomp' and token.head.pos_ == 'VERB' and token.pos_ == 'ADJ':\n",
    "                      if token.text == 'able':\n",
    "                          continue\n",
    "                      #print('Pattern 3: VB', token.head, '->', token.dep_, '-> ADJ', token.text)\n",
    "                      data.at[idx, 'AComp'] = data.at[idx, 'AComp'] + 1\n",
    "\n",
    "          # Max height of the dependency tree of a sentence of a given requirement\n",
    "          data.at[idx, 'DTreeHeight'] = maxHeight\n",
    "        \n",
    "        if \"FinalSel\" in dep_feat_type:\n",
    "          for sent in doc.sents:\n",
    "              for token in sent:\n",
    "                  height = 1\n",
    "                  for t in token.ancestors:\n",
    "                      height = height + 1\n",
    "                  if height > maxHeight:\n",
    "                      maxHeight = height\n",
    "\n",
    "                  if token.dep_ == 'acomp' and token.head.pos_ == 'VERB' and token.pos_ == 'ADJ':\n",
    "                      if token.text == 'able':\n",
    "                          continue\n",
    "                      #print('Pattern 3: VB', token.head, '->', token.dep_, '-> ADJ', token.text)\n",
    "                      data.at[idx, 'AComp'] = data.at[idx, 'AComp'] + 1\n",
    "\n",
    "        for d in significant_dependencies:\n",
    "            if d in req_dep:\n",
    "                data.at[idx, d] = data.at[idx, d] + 1\n",
    "        for c in significant_2dependencies:\n",
    "            if c[0] in req_dep and c[1] in req_dep:\n",
    "                data.at[idx, c[0]+'+'+c[1]] = data.at[idx, c[0]+'+'+c[1]] +1\n",
    "        for t in significant_3dependencies:\n",
    "            if t[0] in req_dep and t[1] in req_dep and t[2] in req_dep:\n",
    "                data.at[idx, t[0]+'+'+t[1]+'+'+t[2]] = data.at[idx, t[0]+'+'+t[1]+'+'+t[2]] +1\n",
    "\n",
    "        for d in significant_branches:\n",
    "            if d in dep_br:\n",
    "                data.at[idx, d] = data.at[idx, d] + 1\n",
    "        for c in significant_2branches:\n",
    "            if c[0] in dep_br and c[1] in dep_br:\n",
    "                data.at[idx, c[0]+'+'+c[1]] = data.at[idx, c[0]+'+'+c[1]] +1\n",
    "        for t in significant_3branches:\n",
    "            if t[0] in dep_br and t[1] in dep_br and t[2] in dep_br:\n",
    "                data.at[idx, t[0]+'+'+t[1]+'+'+t[2]] = data.at[idx, t[0]+'+'+t[1]+'+'+t[2]] +1\n",
    "              \n",
    "        for s in significant_sequences:\n",
    "            if s in req_tagged_seq:\n",
    "              data.at[idx, s] = data.at[idx, s] + 1\n",
    "\n",
    "        idx = idx + 1\n",
    "\n",
    "    if \"ext\" in dep_feat_type:\n",
    "      parser = benepar.Parser(\"benepar_en2\")\n",
    "      data['Modal'] = 0\n",
    "      data['Adjective'] = 0\n",
    "      data['Noun'] = 0\n",
    "      data['Adverb'] = 0\n",
    "      data['Cardinal'] = 0\n",
    "      data['CompSupAdj'] = 0\n",
    "      data['CompSupAdv'] = 0\n",
    "      data['Words'] = 0\n",
    "      data['TreeHeight'] = 0\n",
    "      data['SubTrees'] = 0\n",
    "      idx = 0\n",
    "      for req in tqdm(data['RequirementText'], desc='Parse trees', position=0):\n",
    "          tokens = tokenizer.tokenize(req)\n",
    "          data.at[idx, 'Words'] = len(tokens)\n",
    "          #using nltk here but analogous to universal tags\n",
    "          tags = nltk.pos_tag(tokens)\n",
    "          fd = nltk.FreqDist(tag for (word, tag) in tags)\n",
    "          for key, value in fd.items():\n",
    "              #print (key + \" \" + str(value))\n",
    "              if key==\"MD\":\n",
    "                  data.at[idx, 'Modal'] = value\n",
    "              if key.startswith(\"JJ\"):\n",
    "                  data.at[idx, 'Adjective'] = value\n",
    "              if key.startswith(\"NN\"):\n",
    "                  data.at[idx, 'Noun'] = value\n",
    "              if key==\"RB\":\n",
    "                  data.at[idx, 'Adverb'] = value\n",
    "              if key==\"CD\":\n",
    "                  data.at[idx, 'Cardinal'] = value\n",
    "              if key==\"JJR\" or key==\"JJS\":\n",
    "                  data.at[idx, 'CompSupAdj'] = data.at[idx, 'CompSupAdj'] + value\n",
    "              if key==\"RBR\" or key==\"RBS\":\n",
    "                  data.at[idx, 'CompSupAdv'] = data.at[idx, 'CompSupAdv'] + value\n",
    "          tree = parser.parse(req)\n",
    "          #print (tree.height(), end =\" \")\n",
    "          data.at[idx, 'TreeHeight'] = tree.height()\n",
    "          data.at[idx, 'SubTrees'] = len(tree)\n",
    "          idx = idx + 1 \n",
    "          \n",
    "    if \"FinalSel\" in dep_feat_type:\n",
    "      parser = benepar.Parser(\"benepar_en2\")\n",
    "      data['Modal'] = 0\n",
    "      data['Adverb'] = 0\n",
    "      data['Cardinal'] = 0\n",
    "      idx = 0\n",
    "      for req in tqdm(data['RequirementText'], desc='Parse trees', position=0):\n",
    "          tokens = tokenizer.tokenize(req)\n",
    "          tags = nltk.pos_tag(tokens)\n",
    "          fd = nltk.FreqDist(tag for (word, tag) in tags)\n",
    "          for key, value in fd.items():\n",
    "              #print (key + \" \" + str(value))\n",
    "              if key==\"MD\":\n",
    "                  data.at[idx, 'Modal'] = value\n",
    "              if key==\"RB\":\n",
    "                  data.at[idx, 'Adverb'] = value\n",
    "              if key==\"CD\":\n",
    "                  data.at[idx, 'Cardinal'] = value\n",
    "          idx = idx + 1 \n",
    "          \n",
    "         \n",
    "    # enrichment with features for root verbs (one feature per verb)\n",
    "    if \"verb\" in dep_feat_type:\n",
    "      #first version tried \n",
    "      verbs_features = ['be', 'use', 'interface', 'comply', 'run',\n",
    "                        'allow', 'display', 'send', 'track', 'include', 'notify', 'add', 'assign', 'request', 'record', 'indicate']\n",
    "      #second version \n",
    "      verbs_features = ['be', 'use', 'ensure', 'interface', 'handle', 'take', 'comply', 'run']\n",
    "      #third version\n",
    "      verbs_features = ['be', 'use', 'ensure', 'interface', 'handle', 'take', 'comply', 'run',\n",
    "                        'allow', 'display', 'send', 'track', 'include', 'notify', 'shall', 'add', 'assign', 'generate', 'request',\n",
    "                        'create', 'define', 'record', 'indicate', 'save'\n",
    "                     ]\n",
    "      for verb in verbs_features:\n",
    "        data[verb] = 0\n",
    "        \n",
    "      idx = 0\n",
    "      for req in tqdm(data['RequirementText'], desc='Analyzing verbs', position=0):\n",
    "        newr = req.replace(\"'\", \"\").replace('be able to', '').replace('be capable of', '').replace('provide the ability to', '').replace('be possible to', '')\n",
    "        doc = nlp(newr)\n",
    "        for t in doc:\n",
    "          if t.dep_=='ROOT':\n",
    "            #req_root = lemmatizer(t.orth_, t.pos_)[0]\n",
    "            for verb in verbs_features:\n",
    "              if req_root==verb:\n",
    "                data.at[idx, verb] += 1\n",
    "                break\n",
    "        idx = idx + 1 \n",
    "        \n",
    "    # boolean features for root verbs (each feature takes val 1 if the req contains at least one verb in the corresponding list)\n",
    "    # USED IN THE PAPER AS LAST FEATURE SET\n",
    "    if \"vlist\" in dep_feat_type:\n",
    "      Fverbs = ['allow', 'display', 'send', 'track', 'include', 'notify', 'shall', 'add', 'assign', 'generate', 'request', 'create', 'define', 'record', 'indicate', 'save']\n",
    "      Qverbs = ['be', 'use', 'ensure', 'interface', 'handle', 'take', 'comply', 'run']\n",
    "      \n",
    "      data['hasFverb'] = 0\n",
    "      data['hasQverb'] = 0\n",
    "        \n",
    "      idx = 0\n",
    "      for req in tqdm(data['RequirementText'], desc='Analyzing verbs', position=0):\n",
    "        newr = req.replace(\"'\", \"\").replace('be able to', '').replace('be capable of', '').replace('provide the ability to', '').replace('be possible to', '')\n",
    "        doc = nlp(newr)\n",
    "        for t in doc:\n",
    "          if t.dep_=='ROOT':\n",
    "            #req_root = lemmatizer(t.orth_, t.pos_)[0]\n",
    "            if req_root in Fverbs:\n",
    "              data.at[idx, 'hasFverb'] = 1\n",
    "            if req_root in Qverbs:\n",
    "              data.at[idx, 'hasQverb'] = 1\n",
    "        idx = idx + 1 \n",
    "\n",
    "    # print(data[:30])\n",
    "\n",
    "    #finally save the enriched datasetfile\n",
    "    data.to_csv(new_file_name, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxBqQLz0nkBW"
   },
   "source": [
    "### 3. Datasets enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8648655,
     "status": "ok",
     "timestamp": 1554735303270,
     "user": {
      "displayName": "Davide Dell'Anna",
      "photoUrl": "https://lh5.googleusercontent.com/-dJU__MJ08Ww/AAAAAAAAAAI/AAAAAAAAbsA/YkubKaeFgQo/s64/photo.jpg",
      "userId": "01259263475584376287"
     },
     "user_tz": -120
    },
    "id": "U8B3lgteDbSA",
    "outputId": "97ed26ee-47a1-4eba-fad9-312d2b346b88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory ./ling/ \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './promise-reclass.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-99e731df2f0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdataset_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'promise-reclass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ds2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ds3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dronology'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wasp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'esa-eucl-est'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'leeds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reqview'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'INDcombined'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'8combined'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_source_datasets\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#the features to use to enrich the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-99e731df2f0f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdataset_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'promise-reclass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ds2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ds3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dronology'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wasp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'esa-eucl-est'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'leeds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reqview'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'INDcombined'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'8combined'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_source_datasets\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#the features to use to enrich the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0;34m' \"python-fwf\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 )\n\u001b[0;32m-> 1147\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m   2291\u001b[0m             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2293\u001b[0;31m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2294\u001b[0m         )\n\u001b[1;32m   2295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './promise-reclass.csv'"
     ]
    }
   ],
   "source": [
    "folder_source_datasets = './' #can be an url\n",
    "folder_dest_datasets = './ling/'\n",
    "\n",
    "#creates a folder that will contain the enriched datasets\n",
    "try: \n",
    "    if not os.path.isdir(folder_dest_datasets):\n",
    "      os.mkdir(folder_dest_datasets)\n",
    "except OSError:  \n",
    "    print (\"Creation of the directory %s failed\" % folder_dest_datasets)\n",
    "    exit()\n",
    "else:  \n",
    "    print (\"Successfully created the directory %s \" % folder_dest_datasets)\n",
    "    \n",
    "    \n",
    "dataset_names = ['promise-reclass', 'ds2', 'ds3', 'dronology', 'wasp', 'esa-eucl-est', 'leeds', 'reqview', 'INDcombined', '8combined']\n",
    "datasets = [pd.read_csv(folder_source_datasets+dataset_name+'.csv', engine='python') for dataset_name in dataset_names] \n",
    "\n",
    "#the features to use to enrich the datasets\n",
    "possible_dependencies_feature_sets = ['FinalSel_vlist', 'FinalSel_verb', 'two', 'all', 'allext', 'sd', 'sdext', 'sdsb','sdsbext', 'sdsb8sel02', 'sdsb8sel02ext', 'seqext', 'evext', 'FinalSel', 'FinalSel7']\n",
    "\n",
    "#creat all enriched datasets\n",
    "for i in range(0, len(datasets)):\n",
    "    print('Dataset: '+dataset_names[i])\n",
    "    for t in possible_dependencies_feature_sets:\n",
    "      print(t)\n",
    "      createEnrichedDataset(datasets[i], folder_dest_datasets+dataset_names[i]+'-ling-'+t+'.csv',t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2772
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8178,
     "status": "ok",
     "timestamp": 1554736617438,
     "user": {
      "displayName": "Davide Dell'Anna",
      "photoUrl": "https://lh5.googleusercontent.com/-dJU__MJ08Ww/AAAAAAAAAAI/AAAAAAAAbsA/YkubKaeFgQo/s64/photo.jpg",
      "userId": "01259263475584376287"
     },
     "user_tz": -120
    },
    "id": "SCnk7SyO51La",
    "outputId": "4382e524-2fb2-456a-a579-38322d878a7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: ling/8combined-ling-all.csv (deflated 83%)\n",
      "  adding: ling/8combined-ling-allext.csv (deflated 81%)\n",
      "  adding: ling/8combined-ling-evext.csv (deflated 82%)\n",
      "  adding: ling/8combined-ling-FinalSel7.csv (deflated 76%)\n",
      "  adding: ling/8combined-ling-FinalSel.csv (deflated 76%)\n",
      "  adding: ling/8combined-ling-FinalSel_verb.csv (deflated 80%)\n",
      "  adding: ling/8combined-ling-FinalSel_vlist.csv (deflated 76%)\n",
      "  adding: ling/8combined-ling-sd.csv (deflated 76%)\n",
      "  adding: ling/8combined-ling-sdext.csv (deflated 75%)\n",
      "  adding: ling/8combined-ling-sdsb8sel02.csv (deflated 76%)\n",
      "  adding: ling/8combined-ling-sdsb8sel02ext.csv (deflated 75%)\n",
      "  adding: ling/8combined-ling-sdsb.csv (deflated 78%)\n",
      "  adding: ling/8combined-ling-sdsbext.csv (deflated 76%)\n",
      "  adding: ling/8combined-ling-seqext.csv (deflated 78%)\n",
      "  adding: ling/8combined-ling-two.csv (deflated 74%)\n",
      "  adding: ling/dronology-ling-all.csv (deflated 81%)\n",
      "  adding: ling/dronology-ling-allext.csv (deflated 79%)\n",
      "  adding: ling/dronology-ling-evext.csv (deflated 81%)\n",
      "  adding: ling/dronology-ling-FinalSel7.csv (deflated 73%)\n",
      "  adding: ling/dronology-ling-FinalSel.csv (deflated 73%)\n",
      "  adding: ling/dronology-ling-FinalSel_verb.csv (deflated 78%)\n",
      "  adding: ling/dronology-ling-FinalSel_vlist.csv (deflated 73%)\n",
      "  adding: ling/dronology-ling-sd.csv (deflated 73%)\n",
      "  adding: ling/dronology-ling-sdext.csv (deflated 72%)\n",
      "  adding: ling/dronology-ling-sdsb8sel02.csv (deflated 72%)\n",
      "  adding: ling/dronology-ling-sdsb8sel02ext.csv (deflated 71%)\n",
      "  adding: ling/dronology-ling-sdsb.csv (deflated 75%)\n",
      "  adding: ling/dronology-ling-sdsbext.csv (deflated 73%)\n",
      "  adding: ling/dronology-ling-seqext.csv (deflated 75%)\n",
      "  adding: ling/dronology-ling-two.csv (deflated 70%)\n",
      "  adding: ling/ds2-ling-all.csv (deflated 84%)\n",
      "  adding: ling/ds2-ling-allext.csv (deflated 82%)\n",
      "  adding: ling/ds2-ling-evext.csv (deflated 84%)\n",
      "  adding: ling/ds2-ling-FinalSel7.csv (deflated 77%)\n",
      "  adding: ling/ds2-ling-FinalSel.csv (deflated 77%)\n",
      "  adding: ling/ds2-ling-FinalSel_verb.csv (deflated 81%)\n",
      "  adding: ling/ds2-ling-FinalSel_vlist.csv (deflated 77%)\n",
      "  adding: ling/ds2-ling-sd.csv (deflated 77%)\n",
      "  adding: ling/ds2-ling-sdext.csv (deflated 76%)\n",
      "  adding: ling/ds2-ling-sdsb8sel02.csv (deflated 77%)\n",
      "  adding: ling/ds2-ling-sdsb8sel02ext.csv (deflated 76%)\n",
      "  adding: ling/ds2-ling-sdsb.csv (deflated 79%)\n",
      "  adding: ling/ds2-ling-sdsbext.csv (deflated 77%)\n",
      "  adding: ling/ds2-ling-seqext.csv (deflated 79%)\n",
      "  adding: ling/ds2-ling-two.csv (deflated 75%)\n",
      "  adding: ling/ds3-ling-all.csv (deflated 86%)\n",
      "  adding: ling/ds3-ling-allext.csv (deflated 84%)\n",
      "  adding: ling/ds3-ling-evext.csv (deflated 85%)\n",
      "  adding: ling/ds3-ling-FinalSel7.csv (deflated 80%)\n",
      "  adding: ling/ds3-ling-FinalSel.csv (deflated 80%)\n",
      "  adding: ling/ds3-ling-FinalSel_verb.csv (deflated 84%)\n",
      "  adding: ling/ds3-ling-FinalSel_vlist.csv (deflated 80%)\n",
      "  adding: ling/ds3-ling-sd.csv (deflated 80%)\n",
      "  adding: ling/ds3-ling-sdext.csv (deflated 78%)\n",
      "  adding: ling/ds3-ling-sdsb8sel02.csv (deflated 79%)\n",
      "  adding: ling/ds3-ling-sdsb8sel02ext.csv (deflated 78%)\n",
      "  adding: ling/ds3-ling-sdsb.csv (deflated 81%)\n",
      "  adding: ling/ds3-ling-sdsbext.csv (deflated 79%)\n",
      "  adding: ling/ds3-ling-seqext.csv (deflated 81%)\n",
      "  adding: ling/ds3-ling-two.csv (deflated 78%)\n",
      "  adding: ling/esa-eucl-est-ling-all.csv (deflated 78%)\n",
      "  adding: ling/esa-eucl-est-ling-allext.csv (deflated 76%)\n",
      "  adding: ling/esa-eucl-est-ling-evext.csv (deflated 78%)\n",
      "  adding: ling/esa-eucl-est-ling-FinalSel7.csv (deflated 71%)\n",
      "  adding: ling/esa-eucl-est-ling-FinalSel.csv (deflated 71%)\n",
      "  adding: ling/esa-eucl-est-ling-FinalSel_verb.csv (deflated 76%)\n",
      "  adding: ling/esa-eucl-est-ling-FinalSel_vlist.csv (deflated 72%)\n",
      "  adding: ling/esa-eucl-est-ling-sd.csv (deflated 72%)\n",
      "  adding: ling/esa-eucl-est-ling-sdext.csv (deflated 70%)\n",
      "  adding: ling/esa-eucl-est-ling-sdsb8sel02.csv (deflated 72%)\n",
      "  adding: ling/esa-eucl-est-ling-sdsb8sel02ext.csv (deflated 70%)\n",
      "  adding: ling/esa-eucl-est-ling-sdsb.csv (deflated 73%)\n",
      "  adding: ling/esa-eucl-est-ling-sdsbext.csv (deflated 72%)\n",
      "  adding: ling/esa-eucl-est-ling-seqext.csv (deflated 73%)\n",
      "  adding: ling/esa-eucl-est-ling-two.csv (deflated 70%)\n",
      "  adding: ling/INDcombined-ling-all.csv (deflated 82%)\n",
      "  adding: ling/INDcombined-ling-allext.csv (deflated 80%)\n",
      "  adding: ling/INDcombined-ling-evext.csv (deflated 82%)\n",
      "  adding: ling/INDcombined-ling-FinalSel7.csv (deflated 76%)\n",
      "  adding: ling/INDcombined-ling-FinalSel.csv (deflated 76%)\n",
      "  adding: ling/INDcombined-ling-FinalSel_verb.csv (deflated 80%)\n",
      "  adding: ling/INDcombined-ling-FinalSel_vlist.csv (deflated 76%)\n",
      "  adding: ling/INDcombined-ling-sd.csv (deflated 76%)\n",
      "  adding: ling/INDcombined-ling-sdext.csv (deflated 75%)\n",
      "  adding: ling/INDcombined-ling-sdsb8sel02.csv (deflated 76%)\n",
      "  adding: ling/INDcombined-ling-sdsb8sel02ext.csv (deflated 74%)\n",
      "  adding: ling/INDcombined-ling-sdsb.csv (deflated 77%)\n",
      "  adding: ling/INDcombined-ling-sdsbext.csv (deflated 76%)\n",
      "  adding: ling/INDcombined-ling-seqext.csv (deflated 77%)\n",
      "  adding: ling/INDcombined-ling-two.csv (deflated 74%)\n",
      "  adding: ling/leeds-ling-all.csv (deflated 79%)\n",
      "  adding: ling/leeds-ling-allext.csv (deflated 77%)\n",
      "  adding: ling/leeds-ling-evext.csv (deflated 80%)\n",
      "  adding: ling/leeds-ling-FinalSel7.csv (deflated 69%)\n",
      "  adding: ling/leeds-ling-FinalSel.csv (deflated 69%)\n",
      "  adding: ling/leeds-ling-FinalSel_verb.csv (deflated 75%)\n",
      "  adding: ling/leeds-ling-FinalSel_vlist.csv (deflated 70%)\n",
      "  adding: ling/leeds-ling-sd.csv (deflated 69%)\n",
      "  adding: ling/leeds-ling-sdext.csv (deflated 69%)\n",
      "  adding: ling/leeds-ling-sdsb8sel02.csv (deflated 69%)\n",
      "  adding: ling/leeds-ling-sdsb8sel02ext.csv (deflated 68%)\n",
      "  adding: ling/leeds-ling-sdsb.csv (deflated 72%)\n",
      "  adding: ling/leeds-ling-sdsbext.csv (deflated 71%)\n",
      "  adding: ling/leeds-ling-seqext.csv (deflated 73%)\n",
      "  adding: ling/leeds-ling-two.csv (deflated 66%)\n",
      "  adding: ling/promise-reclass-ling-all.csv (deflated 82%)\n",
      "  adding: ling/promise-reclass-ling-allext.csv (deflated 80%)\n",
      "  adding: ling/promise-reclass-ling-evext.csv (deflated 82%)\n",
      "  adding: ling/promise-reclass-ling-FinalSel7.csv (deflated 75%)\n",
      "  adding: ling/promise-reclass-ling-FinalSel.csv (deflated 75%)\n",
      "  adding: ling/promise-reclass-ling-FinalSel_verb.csv (deflated 80%)\n",
      "  adding: ling/promise-reclass-ling-FinalSel_vlist.csv (deflated 76%)\n",
      "  adding: ling/promise-reclass-ling-sd.csv (deflated 76%)\n",
      "  adding: ling/promise-reclass-ling-sdext.csv (deflated 74%)\n",
      "  adding: ling/promise-reclass-ling-sdsb8sel02.csv (deflated 76%)\n",
      "  adding: ling/promise-reclass-ling-sdsb8sel02ext.csv (deflated 74%)\n",
      "  adding: ling/promise-reclass-ling-sdsb.csv (deflated 77%)\n",
      "  adding: ling/promise-reclass-ling-sdsbext.csv (deflated 75%)\n",
      "  adding: ling/promise-reclass-ling-seqext.csv (deflated 77%)\n",
      "  adding: ling/promise-reclass-ling-two.csv (deflated 74%)\n",
      "  adding: ling/reqview-ling-all.csv (deflated 82%)\n",
      "  adding: ling/reqview-ling-allext.csv (deflated 80%)\n",
      "  adding: ling/reqview-ling-evext.csv (deflated 82%)\n",
      "  adding: ling/reqview-ling-FinalSel7.csv (deflated 73%)\n",
      "  adding: ling/reqview-ling-FinalSel.csv (deflated 73%)\n",
      "  adding: ling/reqview-ling-FinalSel_verb.csv (deflated 78%)\n",
      "  adding: ling/reqview-ling-FinalSel_vlist.csv (deflated 73%)\n",
      "  adding: ling/reqview-ling-sd.csv (deflated 73%)\n",
      "  adding: ling/reqview-ling-sdext.csv (deflated 71%)\n",
      "  adding: ling/reqview-ling-sdsb8sel02.csv (deflated 72%)\n",
      "  adding: ling/reqview-ling-sdsb8sel02ext.csv (deflated 71%)\n",
      "  adding: ling/reqview-ling-sdsb.csv (deflated 75%)\n",
      "  adding: ling/reqview-ling-sdsbext.csv (deflated 73%)\n",
      "  adding: ling/reqview-ling-seqext.csv (deflated 75%)\n",
      "  adding: ling/reqview-ling-two.csv (deflated 70%)\n",
      "  adding: ling/wasp-ling-all.csv (deflated 81%)\n",
      "  adding: ling/wasp-ling-allext.csv (deflated 79%)\n",
      "  adding: ling/wasp-ling-evext.csv (deflated 81%)\n",
      "  adding: ling/wasp-ling-FinalSel7.csv (deflated 73%)\n",
      "  adding: ling/wasp-ling-FinalSel.csv (deflated 73%)\n",
      "  adding: ling/wasp-ling-FinalSel_verb.csv (deflated 77%)\n",
      "  adding: ling/wasp-ling-FinalSel_vlist.csv (deflated 73%)\n",
      "  adding: ling/wasp-ling-sd.csv (deflated 73%)\n",
      "  adding: ling/wasp-ling-sdext.csv (deflated 71%)\n",
      "  adding: ling/wasp-ling-sdsb8sel02.csv (deflated 73%)\n",
      "  adding: ling/wasp-ling-sdsb8sel02ext.csv (deflated 71%)\n",
      "  adding: ling/wasp-ling-sdsb.csv (deflated 75%)\n",
      "  adding: ling/wasp-ling-sdsbext.csv (deflated 73%)\n",
      "  adding: ling/wasp-ling-seqext.csv (deflated 75%)\n",
      "  adding: ling/wasp-ling-two.csv (deflated 71%)\n"
     ]
    }
   ],
   "source": [
    "#to save all in once, since jupyter does not allow to download multiple files together\n",
    "!zip ALL_DATASETS.zip -r ling/*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "05_ling_enrichment.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
